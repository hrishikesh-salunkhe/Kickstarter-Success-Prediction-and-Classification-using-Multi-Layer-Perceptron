# -*- coding: utf-8 -*-
"""FINAL_preprocessing_description.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v21REPhMrifiwOyY55shd-fEjXu-g2EG
"""

from google.colab import drive
drive.mount('/content/gdrive')

!cp "/content/gdrive/My Drive/BE Project/Datasets/kickstarter-description.csv" "/content/kickstarter-description.csv"

!mkdir "/content/gdrive/My Drive/Datasets/"
!mkdir "/content/gdrive/My Drive/Datasets/kickstarter-description-info/"

import pandas as pd
import numpy as np
import sklearn as skl
import matplotlib.pyplot as plt

df = pd.read_csv("kickstarter-description.csv")

df['deadline']= pd.to_datetime(df['deadline'])
df['launched']= pd.to_datetime(df['launched'])

day=[]
pd.Series.dt.day

df['dealine_day'] = df['deadline'].dt.day
pd.Series.dt.month 
df['deadline_month'] = df['deadline'].dt.month

pd.Series.dt.year 
df['deadline_year'] = df['deadline'].dt.year

day=[]
pd.Series.dt.day

df['launched_day'] = df['launched'].dt.day
pd.Series.dt.month 
df['launched_month'] = df['launched'].dt.month

pd.Series.dt.year 
df['launched_year'] = df['launched'].dt.year

df = df.set_index("state")
df = df.drop(['canceled', 'live', 'undefined', 'suspended'], axis=0)

df = df.reset_index()

df

"""# **Unique values function**"""

def unique(list1): 
  
    # intilize a null list 
    unique_list = [] 
      
    # traverse for all elements 
    for x in list1: 
        # check if exists in unique_list or not 
        if x not in unique_list: 
            unique_list.append(x) 
        
    return unique_list

unique(df["category"])

"""# ***Load the arrays if preprocessing has already been done***"""

state = np.load('/content/gdrive/My Drive/Datasets/kickstarter-description-info/state.npy')
main_category = np.load('/content/gdrive/My Drive/Datasets/kickstarter-description-info/main_category.npy')
category = np.load('/content/gdrive/My Drive/Datasets/kickstarter-description-info/category.npy')
country = np.load('/content/gdrive/My Drive/Datasets/kickstarter-description-info/country.npy')
currency = np.load('/content/gdrive/My Drive/Datasets/kickstarter-description-info/currency.npy')
duration = np.load('/content/gdrive/My Drive/Datasets/kickstarter-description-info/duration.npy')

"""# **Preprocessing Starts!**"""

state = []

for i in range(0,len(df)):
  if(df.state[i]=="failed"):
    state.append(0)
  else:
    state.append(1)
    
print(state)

main_category =[]

unique_list = unique(df["main_category"])

for i in range(0,len(df)):
  for j in range(0, len(unique_list)):
    if(df["main_category"][i]==unique_list[j]):
      main_category.append(j)
      
print(main_category)

category =[]

unique_list = unique(df["category"])

for i in range(0,len(df)):
  for j in range(0, len(unique_list)):
    if(df["category"][i]==unique_list[j]):
      category.append(j)
      
print(category)

country =[]

unique_list = unique(df["country"])

for i in range(0,len(df)):
  for j in range(0, len(unique_list)):
    if(df["country"][i]==unique_list[j]):
      country.append(j)
      
print(country)

currency =[]

unique_list = unique(df["currency"])

for i in range(0,len(df)):
  for j in range(0, len(unique_list)):
    if(df["currency"][i]==unique_list[j]):
      currency.append(j)
      
print(currency)

duration=[]

for i in range (0,len(df)):
  if (df.iloc[i][35] > df.iloc[i][38]):
    n_months = 12 + df.iloc[i][34] - df.iloc[i][37]
    duration_temp = 30 - df.iloc[i][36] + (n_months-1)*31 + df.iloc[i][33]
  else:
    n_months = df.iloc[i][34] - df.iloc[i][37]
    duration_temp = 30 - df.iloc[i][36] + (n_months-1)*31 + df.iloc[i][33]
  duration.append(duration_temp)
      
print(duration)

np.save('/content/gdrive/My Drive/Datasets/kickstarter-description-info/state.npy', state)
np.save('/content/gdrive/My Drive/Datasets/kickstarter-description-info/main_category.npy', main_category)
np.save('/content/gdrive/My Drive/Datasets/kickstarter-description-info/category.npy', category)
np.save('/content/gdrive/My Drive/Datasets/kickstarter-description-info/country.npy', country)
np.save('/content/gdrive/My Drive/Datasets/kickstarter-description-info/currency.npy', currency)
np.save('/content/gdrive/My Drive/Datasets/kickstarter-description-info/duration.npy', duration)

"""# **Merging the arrays into the main dataframe**"""

df_temp = df[['ID', 'n_words', 'n_sents', 'n_chars', 'n_syllables',
       'n_unique_words', 'n_long_words', 'n_monosyllable_words','n_polysyllable_words',
       'flesch_kincaid_grade_level', 'flesch_reading_ease', 'smog_index',
       'gunning_fog_index', 'coleman_liau_index',
       'automated_readability_index', 'lix', 'gulpease_index',
       'wiener_sachtextformel', 'goal', 'pledged', 'backers']]

data = {'ID': df_temp["ID"], 'state': state, 'category': category, 'main category': main_category, 'country': country, 'currency': currency, 'duration': duration}

df_array = pd.DataFrame(data)
df_final = pd.merge(df_temp,df_array,on="ID")
df_final

"""# **Removing null and negative values from the final dataset**"""

df_final = df_final.fillna(99999)
df_final[df_final < 0] = 0

df_final.corr()

"""# OUTLIER DETECTION:

Enter the number of rounds of Outlier Removal to be performed below:
"""

n_rounds = 3

def find_anomalies(random_data):
    anomalies =[]
    
    # Set upper and lower limit to 3 standard deviation
    random_data_std = np.std(random_data)
    random_data_mean = np.mean(random_data)
    anomaly_cut_off = random_data_std * 3
    
    lower_limit  = random_data_mean - anomaly_cut_off 
    upper_limit = random_data_mean + anomaly_cut_off
    #print(lower_limit)
    # Generate outliers
    for outlier in random_data:
        if outlier > upper_limit or outlier < lower_limit:
            anomalies.append(outlier)
    return anomalies

for i in range(n_rounds):
  # 1: GOAL

  temp = unique(find_anomalies(df_final['goal']))

  df_final = df_final.set_index("goal")
  df_final = df_final.drop(temp, axis = 0)

  df_final = df_final.reset_index()

  # # 2: COUNTRY

  # temp = unique(find_anomalies(df_final['country']))

  # df_final = df_final.set_index("country")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: DURATION

  # temp = unique(find_anomalies(df_final['duration']))

  # df_final = df_final.set_index("duration")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: n_words

  # temp = unique(find_anomalies(df_final['n_words']))

  # df_final = df_final.set_index("n_words")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: n_sents

  # temp = unique(find_anomalies(df_final['n_sents']))

  # df_final = df_final.set_index("n_sents")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: n_chars

  # temp = unique(find_anomalies(df_final['n_chars']))

  # df_final = df_final.set_index("n_chars")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: n_syllables

  # temp = unique(find_anomalies(df_final['n_syllables']))

  # df_final = df_final.set_index("n_syllables")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: n_unique_words

  # temp = unique(find_anomalies(df_final['n_unique_words']))

  # df_final = df_final.set_index("n_unique_words")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: n_long_words

  # temp = unique(find_anomalies(df_final['n_long_words']))

  # df_final = df_final.set_index("n_long_words")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: n_monosyllable_words

  # temp = unique(find_anomalies(df_final['n_monosyllable_words']))

  # df_final = df_final.set_index("n_monosyllable_words")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: n_polysyllable_words

  # temp = unique(find_anomalies(df_final['n_polysyllable_words']))

  # df_final = df_final.set_index("n_polysyllable_words")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: flesch_kincaid_grade_level

  # temp = unique(find_anomalies(df_final['flesch_kincaid_grade_level']))

  # df_final = df_final.set_index("flesch_kincaid_grade_level")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: flesch_reading_ease

  # temp = unique(find_anomalies(df_final['flesch_reading_ease']))

  # df_final = df_final.set_index("flesch_reading_ease")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: smog_index

  # temp = unique(find_anomalies(df_final['smog_index']))

  # df_final = df_final.set_index("smog_index")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: gunning_fog_index

  # temp = unique(find_anomalies(df_final['gunning_fog_index']))

  # df_final = df_final.set_index("gunning_fog_index")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: coleman_liau_index

  # temp = unique(find_anomalies(df_final['coleman_liau_index']))

  # df_final = df_final.set_index("coleman_liau_index")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: automated_readability_index

  # temp = unique(find_anomalies(df_final['automated_readability_index']))

  # df_final = df_final.set_index("automated_readability_index")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: lix

  # temp = unique(find_anomalies(df_final['lix']))

  # df_final = df_final.set_index("lix")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: wiener_sachtextformel

  # temp = unique(find_anomalies(df_final['wiener_sachtextformel']))

  # df_final = df_final.set_index("wiener_sachtextformel")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: category

  # temp = unique(find_anomalies(df_final['category']))

  # df_final = df_final.set_index("category")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: main category

  # temp = unique(find_anomalies(df_final['main category']))

  # df_final = df_final.set_index("main category")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()

  # # 3: currency

  # temp = unique(find_anomalies(df_final['currency']))

  # df_final = df_final.set_index("currency")
  # df_final = df_final.drop(temp, axis = 0)

  # df_final = df_final.reset_index()


  print('ROUND ', i+1)
  print('Number of rows left in the dataset: ', len(df_final))

df_final.corr()

#           ID	   n_words	n_sents	   n_chars	n_syllables	n_unique_words	n_long_words	n_monosyllable_words	n_polysyllable_words	flesch_kincaid_grade_level	flesch_reading_ease	smog_index	gunning_fog_index	coleman_liau_index	automated_readability_index	lix	  gulpease_index	wiener_sachtextformel	goal	  pledged	  backers	   state	   category	main category	 country	 currency	   duration
# state	-0.000722	0.112060	0.057997	0.097500	  0.094644	  0.115576	     0.035043	         0.094198	            0.009423	              -0.041152	                   0.031554	     0.001410	     -0.029659	       -0.025047	           -0.057420	         -0.042526	-0.069127      	  -0.053894	     -0.025099	0.109507	0.125790	1.000000	-0.041316	 -0.037480	  -0.050306	 -0.056377	-0.115071

#        currency	main category	category	wiener_sachtextformel  	lix	  automated_readability_index	coleman_liau_index	gunning_fog_index	smog_index	flesch_reading_ease	flesch_kincaid_grade_level	n_polysyllable_words	n_monosyllable_words	n_long_words	n_unique_words	n_syllables	n_chars	  n_sents	  n_words	   duration	 country	  goal	     ID	    gulpease_index	pledged	backers	   state
# state	-0.014127	 -0.018727	 -0.032700	    -0.007354	       0.002250	       -0.003966	              0.019176	           0.015372	     0.017721	       -0.005822	           0.007427	                0.024184	             0.077265	         0.043011	       0.100559	      0.088618	0.090726	0.056830	0.097395	-0.114249	-0.014127	-0.174985	-0.000988	  -0.057278	   0.127019	0.154316	1.000000

#      smog_index	n_polysyllable_words	n_long_words	goal      	ID   	n_words  	n_sents  	n_chars	n_syllables	n_unique_words	n_monosyllable_words	flesch_kincaid_grade_level	flesch_reading_ease	gunning_fog_index	coleman_liau_index	automated_readability_index	  lix	   gulpease_index	wiener_sachtextformel	pledged	backers	   state	  category	main category	country	   currency 	duration
# state	0.000881	     0.009634	         0.036235	 -0.175735	-0.001255	0.110989	0.058389	0.099620	0.098069    	0.114478	       0.092896	               -0.041541               	0.032289	         -0.031097	        -0.024962	             -0.057324	         -0.043378	 -0.068682        	-0.056017	     0.125097	0.140297	1.000000	-0.040212	  -0.035570	 -0.046628	-0.052065	 -0.110460

"""# **Implementing Classifier**

# **Implementing Classifier**
"""

a = df_final['n_words'].values
b = df_final['n_sents'].values
c = df_final['n_chars'].values
d = df_final['n_syllables'].values
e = df_final['n_unique_words'].values
f = df_final['n_long_words'].values
g = df_final['n_monosyllable_words'].values
h = df_final['n_polysyllable_words'].values
i = df_final['flesch_kincaid_grade_level'].values
j = df_final['flesch_reading_ease'].values
k = df_final['smog_index'].values
l = df_final['gunning_fog_index'].values
m = df_final['coleman_liau_index'].values
n = df_final['automated_readability_index'].values
o = df_final['lix'].values
p = df_final['wiener_sachtextformel'].values
q = df_final['goal'].values
r = df_final['category'].values
s = df_final['main category'].values
t = df_final['country'].values
u = df_final['currency'].values
v = df_final['duration'].values

X = np.array([a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v])
X = X.transpose()
print(X.shape)

np.savetxt("kickstarter_description_inputs.csv", X, delimiter=",")
!cp "/content/kickstarter_description_inputs.csv" "/content/gdrive/My Drive/BE Project/Datasets/kickstarter-description-info/kickstarter_description_inputs.csv"

y_temp = df_final['state'].values
y = np.array([y_temp])
y = y.transpose()
y = y.ravel()
print(y.shape)

np.savetxt("kickstarter_description_labels.csv", y, delimiter=",")
!cp "/content/kickstarter_description_labels.csv" "/content/gdrive/My Drive/BE Project/Datasets/kickstarter-description-info/kickstarter_description_labels.csv"

"""# ***Generating a csv file of all the final values to be used***"""

data_final = {'n_words': a, 'n_sents': b, 'n_chars': c, 'n_syllables': d, 'n_unique_words': e, 'n_long_words': f, 'n_monosyllable_words': g, 'n_polysyllable_words': h, 
'flesch_kincaid_grade_level': i, 'flesch_reading_ease': j, 'smog_index': k, 'gunning_fog_index': l, 'coleman_liau_index': m, 'automated_readability_index': n,
'lix': o, 'wiener_sachtextformel': p, 'goal': q, 'category': r, 'main category': s, 'country': t, 'currency': u, 'duration': v, 'state': y_temp }

data_final_lr = {'n_words': a, 'n_sents': b, 'n_chars': c, 'n_syllables': d, 'n_unique_words': e, 'n_long_words': f, 'n_monosyllable_words': g, 'n_polysyllable_words': h, 
'flesch_kincaid_grade_level': i, 'flesch_reading_ease': j, 'smog_index': k, 'gunning_fog_index': l, 'coleman_liau_index': m, 'automated_readability_index': n,
'lix': o, 'wiener_sachtextformel': p, 'goal': q, 'pledged': df_final['pledged'].values, 'backers': df_final['backers'].values, 'category': r, 'main category': s, 'country': t, 'currency': u, 'duration': v, 'state': y_temp }

df_array_final_lr = pd.DataFrame(data_final_lr)
df_array_final_lr_sample = df_array_final_lr.head(1000)

df_array_final = pd.DataFrame(data_final)
df_array_final_sample = df_array_final.head(1000)

df_array_final_lr.to_csv (r'kickstarter_description_final_lr_header.csv', index = False, header=True)
df_array_final_lr_sample.to_csv (r'kickstarter_description_final_lr_sample_header.csv', index = False, header=True)

df_array_final.to_csv (r'kickstarter_description_final.csv', index = False, header=False)
df_array_final_sample.to_csv (r'kickstarter_description_final_sample.csv', index = False, header=False)

df_array_final.to_csv (r'kickstarter_description_final_header.csv', index = False, header=True)
df_array_final_sample.to_csv (r'kickstarter_description_final_sample_header.csv', index = False, header=True)

!cp "/content/kickstarter_description_final.csv" "/content/gdrive/My Drive/BE Project/Datasets/kickstarter-description-info/kickstarter_description_final.csv"
!cp "/content/kickstarter_description_final_sample.csv" "/content/gdrive/My Drive/BE Project/Datasets/kickstarter-description-info/kickstarter_description_final_sample.csv"

!cp "/content/kickstarter_description_final_header.csv" "/content/gdrive/My Drive/BE Project/Datasets/kickstarter-description-info/kickstarter_description_final_header.csv"
!cp "/content/kickstarter_description_final_sample_header.csv" "/content/gdrive/My Drive/BE Project/Datasets/kickstarter-description-info/kickstarter_description_final_sample_header.csv"

!cp "/content/kickstarter_description_final_lr_header.csv" "/content/gdrive/My Drive/BE Project/Datasets/kickstarter-description-info/kickstarter_description_final_lr_header.csv"
!cp "/content/kickstarter_description_final_lr_sample_header.csv" "/content/gdrive/My Drive/BE Project/Datasets/kickstarter-description-info/kickstarter_description_final_lr_sample_header.csv"